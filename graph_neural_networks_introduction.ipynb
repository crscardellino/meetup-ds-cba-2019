{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"font-size:2.5em;text-align:center;\">Introducción a redes neuronales sobre grafos</h1>\n",
    "<h2 style=\"font-size:1.5em;text-align:center;\">Cristian Cardellino - FAMAFyC - UNC</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contenido\n",
    "\n",
    "1. [Introducción y motivación](#Introducción-y-motivación)\n",
    "1. [Definiciones](#Definiciones)\n",
    "1. [Tipos de redes sobre grafos](#Tipos-de-redes-sobre-grafos)\n",
    "1. [Mecanismos y tareas](#Mecanismos-y-tareas)\n",
    "1. [Aprendizaje Semi-supervisado con GCNs](#Aprendizaje-Semi-supervisado-con-GCNs)\n",
    "1. [Ejemplo: Zachary's Karate Club](#Ejemplo:-Zachary's-Karate-Club)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Credits\n",
    "\n",
    "These slides are inspired on the work by Wu et al.: [\"A comprehensive survey on graph neural networks\"](https://arxiv.org/abs/1901.00596) [1] and the article by Thomas Kipf on [Graph Convolutional Networks](https://tkipf.github.io/graph-convolutional-networks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducción y motivación\n",
    "\n",
    "- Muchos conjuntos de datos tienen estructuras de grafos.\n",
    "    - Redes sociales, grafos de conocimiento, World Wide Web.\n",
    "- El estado del arte en clasificación de grafos son métodos de kernel.\n",
    "- En los últimos años varios han revisado el problema de generalizar redes neuronales a las estructuras arbitrarias de los grafos.\n",
    "    - Se han encontrado aplicaciones tareas de NLP [2], CV [3], predicción de tráfico [4], sistemas de recomendación [5] o química [6].\n",
    "    - Google publicó un artículo sobre como se utilizan GNN para [detectar propiedades olfativas en las moléculas](https://ai.googleblog.com/2019/10/learning-to-smell-using-deep-learning.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definiciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Grafos\n",
    "\n",
    "- Un grafo se representa por un par ordenado: $G = (V, E)$.\n",
    "    - $V$ es un conjunto de vértices o nodos.\n",
    "    - $E$ es un conjunto de aristas: $e_{ij} = (v_i, v_j) \\in E$ con  $v_i, v_j \\in V$.\n",
    "- La **vecindad de un nodo** se define como: $N(v) = \\{u \\in V | (v, u) \\in E\\}$.\n",
    "- La **matriz de adyacencia** $\\textbf{A} \\in \\mathbb{R}^{n \\times n}$ ($n = |V|$) con $A_{ij} = 1$ si $e_{ij} \\in E$ y $A_{ij} = 0$ si $e_{ij} \\not \\in E$.\n",
    "- Un grafo puede tener **atributos de nodo** $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, donde $\\mathbf{x}_v \\in \\mathbb{R}^d$ representa el vector de atributos del nodo $v$.\n",
    "- Un grafo puede tener **atributos de aristas** $\\mathbf{X}^e \\in \\mathbb{R}^{n \\times c}$, donde $\\mathbf{x}^e_{v,u} \\in \\mathbb{R}^c$ representa el vector de la arista $(v, u)$.\n",
    "- Si el grafo es **no dirigido**, $e_{ij} \\in E \\iff e_{ji} \\in E$.\n",
    "- Un grafo **espacio temporal** es un grafo donde los nodos de los atributos cambian dinámicamente en el tiempo. Se define como $G^{(t)} = (\\mathbf{V}, \\mathbf{E}, \\mathbf{X}^{(t)})$ con $\\mathbf{X}^{(t)} \\in \\mathbb{R}^{n \\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Red neuronal sobre grafo\n",
    "\n",
    "Sean, \n",
    "\n",
    "- $G = (V, E)$ un grafo, \n",
    "- $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ la matriz de adyacencia de $E$, \n",
    "- y $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ una matriz de características de los nodos de $V$, \n",
    "\n",
    "se define una red neuronal sobre los nodos de un grafo a aquella que produce como salida una matriz $\\mathbf{Z} \\in \\mathbb{R}^{n \\times m}$ done $m$ es la cantidad de atributos de salida de un nodo.\n",
    "\n",
    "Una capa neuronal sobre el grafo se define como una función no lineal,\n",
    "\n",
    "$$\n",
    "    H^{(l+1)} = f\\big(H^{(l)}, \\mathbf{A}\\big)\n",
    "$$\n",
    "\n",
    "que de manera tal que $H^{(0)} = \\mathbf{X}$ y $H^{(L)} = \\mathbf{Z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tipos de redes sobre grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redes recurrentes sobre grafos\n",
    "\n",
    "- Son pioneras en el uso de redes neuronales en grafos.\n",
    "- Objetivo: aprender la representación de un nodo con una arquitectura recurrente.\n",
    "- Asumen que un nodo intercambia información constantemente con sus vecinos.\n",
    "- Aplican el mismo conjunto de parámetros de manera recurrente sobre los nodos de un grafo para obtener representaciones de alto nivel.\n",
    "\n",
    "<img alt=\"RecGNN\" src=\"./img/rec-gnn.png\" style=\"width:80%;margin:auto;\"/>\n",
    "<p style=\"font-size:1.5rem;text-align:right;\">Image credits: Wu et al. [1]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redes convolucionales sobre grafos\n",
    "\n",
    "- Generalizan la operación de convolución de una \"red\" a un grafo.\n",
    "- Generan una representación de características de un nodo $v$ mediante la agregación de sus propios atributos $\\mathbf{x}_v$ y los de sus vecions $\\{\\mathbf{x}_u | u \\in N(v)\\}$.\n",
    "- A diferencia de las redes recurrentes, las convolucionales apilan capas para extraer representaciones de alto nivel de los nodos.\n",
    "- Han jugado un rol primario en construir varios modelos complejos de GNNs.\n",
    "\n",
    "\n",
    "<img alt=\"ConvGNN\" src=\"./img/conv-gnn.png\" style=\"width:80%;margin:auto;\"/>\n",
    "<p style=\"font-size:1.5rem;text-align:right;\">Image credits: Wu et al. [1]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Autoencoders sobre grafos\n",
    "\n",
    "- Se utilizan como método de aprendizaje no supervisado.\n",
    "- Codifican nodos/grafos en un espacio vectorial de valores latentes.\n",
    "- Reconstrucyen la información del grafo a partir de la información codificada.\n",
    "- Utilizados para aprender embeddings de redes y distribuciones generativas de grafos.\n",
    "\n",
    "\n",
    "<img alt=\"GAE\" src=\"./img/gae.png\" style=\"width:80%;margin:auto;\"/>\n",
    "<p style=\"font-size:1.5rem;text-align:right;\">Image credits: Wu et al. [1]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redes neuronales sobre grafos espacio temporales\n",
    "\n",
    "- Buscan aprender representaciones para grafos espacio temporales.\n",
    "- Muy utilizadas en predicción de velocidad sobre tráfico [4], anticipación de maniobras de conductores [7] o reconocimiento de acciones humanas [3].\n",
    "- Buscan considerar la dependencia espacial y temporal al mismo tiempo.\n",
    "    - Varias aplicaciones suelen integrar GNNs para caputrar la dependencia espacial junto a una RNN (o CNN) que capture la dependencia temporal.\n",
    "\n",
    "\n",
    "<img alt=\"STGNN\" src=\"./img/stgnn.png\" style=\"width:80%;margin:auto;\"/>\n",
    "<p style=\"font-size:1.5rem;text-align:right;\">Image credits: Wu et al. [1]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mecanismos y tareas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mecanismos\n",
    "\n",
    "Las redes sobre grafos pueden se aplicadas en distintos niveles en base a la tarea en cuestión. Los mecanismos para utilizarlas pueden ser los siguientes:\n",
    "\n",
    "- **Nodo**: Son tareas de regresión o clasificación de nodos. Utilizan una capa densa como salida (con posible softmax) para clasificar los nodos.\n",
    "    - E.g. analisis de sentimiento en redes sociales.\n",
    "- **Arista**: Tareas de clasificación y predicción de vínculos. Buscan encontrar conexiones entre vértices y/o peso en una arista.\n",
    "    - E.g. extracción de relaciones entre entidades.\n",
    "- **Grafo**: Tareas de clasificación sobre grafos.\n",
    "    - E.g. clasificación de grafos moleculares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tareas: Aprendizaje Semi-supervisado\n",
    "\n",
    "- En el trabajo a nivel nodo, se pueden trabajar tareas semi-supervisadas.\n",
    "- Dada una red con nodos parcialmente anotados, las redes convolucionales sobre grafos pueden aprender modelos robustos que efectivamente identifican las etiquetas de los nodos no anotados.\n",
    "\n",
    "<img alt=\"Semi-supervised GCN\" src=\"./img/conv-gnn-node-level.png\" style=\"width:80%;margin:auto;\"/>\n",
    "<p style=\"font-size:1.5rem;text-align:right;\">Image credits: Wu et al. [1]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tareas: Aprendizaje Supervisado\n",
    "\n",
    "- Con grafos anotados, se pueden entrenar modelos para predecir la clase de cierto grafo.\n",
    "- Se utiliza una combinación de redes convolucionales sobre grafos, operaciones de *pooling*, y operaciones de *readout*.\n",
    "- Con una operación sobre capas densas al final y softmax se puede obtener un framework para clasificación.\n",
    "\n",
    "<img alt=\"Supervised GCN\" src=\"./img/conv-gnn-graph-level.png\" style=\"width:80%;margin:auto;\"/>\n",
    "<p style=\"font-size:1.5rem;text-align:right;\">Image credits: Wu et al. [1]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tareas: Aprendizaje No Supervisado\n",
    "\n",
    "- Se explota la información de las aristas de manera no supervisada.\n",
    "    - Puede aplicarse mediante un autoencoder sobre grafos.\n",
    "    - Otra opción es la utilización de muestreo negativo sobre aristas existentes/no existentes.\n",
    "- Se utilizan para encontrar representaciones de redes (i.e. network embeddings) o bien para la generación de grafos (utilizadas para resolver generación de grafos moleculares)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aprendizaje Semi-supervisado con GCNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Graph convolutional networks\n",
    "\n",
    "- Son un tipo particular de redes convolucionales sobre grafos diseñadas por Kipf & Welling [2].\n",
    "- Presentan un modelo bastante simple que es aplicable de manera eficiente a problemas de índole semi-supervisado (e.g. que tengan nodos parcialmente anotados).\n",
    "- Pueden pensarse como una generalización de las CNN a un grafo que no esté conectado a modo de cuadrícula.\n",
    "\n",
    "<img alt=\"GCN\" src=\"./img/conv-vs-gcn.png\" style=\"width:60%;margin:auto;\"/>\n",
    "<p style=\"font-size:1.5rem;text-align:right;\">Image credits: Wu et al. [1]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Capa convolucional\n",
    "\n",
    "Sean,\n",
    "\n",
    "- $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ una matriz de adyacencia,\n",
    "- $H^{(l)}$ una representación intermedia,\n",
    "- $W^{(l)}$ una matriz de pesos para la capa $l$,\n",
    "- $\\sigma$ una función de activación no lineal,\n",
    "\n",
    "se define una capa convolucional como,\n",
    "\n",
    "$$\n",
    "    f\\big(H^{(l)}, \\mathbf{A}\\big) = \\sigma \\big(\\mathbf{A}H^{(l)}W^{(l)}\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Limitaciones \n",
    "\n",
    "La capa convolucional como se define en el paso anterior tiene dos limitaciones:\n",
    "\n",
    "1. Al multiplicar por $\\mathbf{A}$ se están sumando los vectores de características de todos los nodos vecinos pero no del mismo nodo (a menos que haya nodos que se autoreferencien).\n",
    "1. Otro problema es que $\\mathbf{A}$ no está normalizada y puede cambiar la escala de los vectores de atributos (e.g. aquellos nodos con más conexiones tendrán valores mayores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Soluciones \n",
    "\n",
    "Para sobrevenir estas limitaciones, Kipf & Welling [2] propusieron las siguientes soluciones sencillas pero efectivas:\n",
    "\n",
    "1. Se fuerza el ciclo sobre los mismos nodos sumando la matriz identidad $I$, luego tenemos una operación sobre una matriz $\\mathbf{\\hat{A}} = \\mathbf{A} + I$.\n",
    "1. Se multiplica $\\mathbf{A}$ por la matriz diagonal $D$ que tiene el grado de los nodos, i.e. $|N(v_i)| = d_{i} \\in D$.\n",
    "\n",
    "En base a esto, se redefine la capa convolucional sobre nodos (GCN) de la siguiente forma:\n",
    "\n",
    "$$\n",
    "    f\\big(H^{(l)}, \\mathbf{A}\\big) = \\sigma \\Big(\\hat{D}^{-\\frac{1}{2}}\\mathbf{\\hat{A}}\\hat{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)} \\Big)\n",
    "$$\n",
    "\n",
    "donde $\\hat{D}$ es la matriz de grado de la matriz $\\mathbf{\\hat{A}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo: Zachary's Karate Club\n",
    "\n",
    "Este es un ejemplo de una red social universitaria, que proviene de un trabajo antropológico de Wayne W. Zachary [8]. Estudia la estructura de una comunidad de 34 miembros de un club de karate con información sobre los miembros que interactuaban fuera del club.\n",
    "\n",
    "Luego de un conflicto entre el instructor \"Mr. Hi\" y el administrador \"John A\", se dividió el club en 2, aquellos que siguieron a \"Mr. Hi\" a un nuevo club y aquellos que se quedaron en el club administrado por \"John A\".\n",
    "\n",
    "Utilizando este ejemplo de base, y sólamente con información de quién es \"John A\" y quién \"Mr. Hi\" en el grafo, desarrollaremos un ejemplo de aprendizaje semi-supervisado con GCNs en base a la estructura del grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from layers import GraphConvolution\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from metrics import masked_accuracy, masked_softmax_cross_entropy\n",
    "from utils import create_mask, load_zachary_karate_club_data, plot_graph\n",
    "from IPython.display import HTML\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cargando los datos\n",
    "\n",
    "Utilizamos `networkx` para la carga del grafo, a partir de la función `load_zachary_karate_club_data` obtenemos el grafo, nuestros datos (en este caso los nodos son representados por una matriz identidad, i.e. no tienen atributos), nuestras etiquetas y la matriz de adyacencia.\n",
    "\n",
    "Graficamos el grafo, los valores celestes son miembros del club de \"Mr. Hi\" y los naranjas son del club de \"John A\". Por otra parte, \"Mr. Hi\" y \"John A\" están identificados por los nodos de color más oscuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "G, data, target, adjacency = load_zachary_karate_club_data()\n",
    "\n",
    "N = len(G)\n",
    "MR_HI = 0\n",
    "JOHN_A = N - 1\n",
    "\n",
    "plot_graph(G, target.argmax(axis=1), plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Dividimos los datos en entrenamiento y evaluación. Los datos de entrenamiento, en este caso, son solo dos, los nodos que representan a \"Mr. Hi\" y a \"John A\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "shufled_indices = np.random.permutation(\n",
    "    np.arange(MR_HI+1, JOHN_A)\n",
    ")\n",
    "\n",
    "train_mask = create_mask(\n",
    "    [MR_HI, JOHN_A],\n",
    "    N\n",
    ")\n",
    "\n",
    "test_mask = create_mask(\n",
    "    shufled_indices[:10],\n",
    "    N\n",
    ")\n",
    "\n",
    "y_train = np.zeros_like(target)\n",
    "y_train[train_mask, :] = target[train_mask, :]\n",
    "\n",
    "y_test = np.zeros_like(target)\n",
    "y_test[test_mask, :] = target[test_mask, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Construcción del modelo\n",
    "\n",
    "Este será un modelo sencillo, de una solamente dos capas convolucionales sobre grafos. Para ello usamos la implementación `GraphConvolution` basada en el trabajo de Kipf y Welling [2]. \n",
    "\n",
    "**Nota:** Esta capa es una versión muy simplificada, recomiendo revisar el [código de Kipf](https://github.com/tkipf/gcn) para una implementación completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = N\n",
    "HIDDEN_SIZE = 10\n",
    "OUTPUT_SIZE = 2\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "REG_PARAMETER = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape=(INPUT_SIZE,))\n",
    "\n",
    "hidden_layer = GraphConvolution(\n",
    "    units=HIDDEN_SIZE,\n",
    "    adjacency=adjacency,\n",
    "    activation=\"relu\",\n",
    "    use_bias=True,\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(REG_PARAMETER),\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    bias_initializer=\"zeros\"\n",
    ")(input_layer)\n",
    "\n",
    "output_layer = GraphConvolution(\n",
    "    units=OUTPUT_SIZE,\n",
    "    adjacency=adjacency,\n",
    "    use_bias=True,\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(REG_PARAMETER),\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    bias_initializer=\"zeros\"\n",
    ")(hidden_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Funciones de entrenamiento y evaluación de TF 2.0\n",
    "\n",
    "Hacemos uso (y abuso) de TF 2.0 para utilizar sus `GradientTape`s que facilitan bastante el trabajo de debug. Generamos una función para entrenamiento y otra para evaluación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_function():\n",
    "    @tf.function\n",
    "    def train_step(data, target, model, mask, loss, optimizer, accuracy):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(data)\n",
    "            cost = loss(target, logits, mask)\n",
    "        grads = tape.gradient(cost, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        return cost, accuracy(target, logits, mask)\n",
    "\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluation_function(data, target, model, mask, loss, accuracy):\n",
    "    logits = model(data)\n",
    "    cost = loss(target, logits, mask)\n",
    "\n",
    "    return cost, masked_accuracy(target, logits, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Con el modelo listo, podemos pasar a entrenar por épocas. Mostramos la información pertinente en cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "train_step = train_function()\n",
    "graph_propagation = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_accuracy = train_step(\n",
    "        data,\n",
    "        y_train,\n",
    "        model,\n",
    "        train_mask,\n",
    "        masked_softmax_cross_entropy,\n",
    "        optimizer,\n",
    "        masked_accuracy\n",
    "    )\n",
    "    \n",
    "    graph_propagation.append(tf.nn.softmax(model(data)).numpy().argmax(axis=1))\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.03f} - Train Accuracy: {train_accuracy:.03f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluación\n",
    "\n",
    "Una vez terminado el entrenamiento procedemos a evaluar en el conjunto de evaluación (que es solo una parte del conjunto total de los datos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluation_function(\n",
    "    data, \n",
    "    y_test, \n",
    "    model, \n",
    "    test_mask, \n",
    "    masked_softmax_cross_entropy, \n",
    "    masked_accuracy\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.03f} - Test Accuracy: {test_accuracy:.03f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Revisión\n",
    "\n",
    "Como este no es un problema realmente semi-supervisado, podemos revisar que tan bien anda el modelo (sólo entrenado con dos nodos) sobre el conjunto total de nodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "overall_loss, overall_accuracy = evaluation_function(\n",
    "    data, \n",
    "    target, \n",
    "    model, \n",
    "    np.ones(target.shape[0], dtype=np.bool), \n",
    "    masked_softmax_cross_entropy, \n",
    "    masked_accuracy\n",
    ")\n",
    "\n",
    "print(f\"Overall Loss: {overall_loss:.03f} - Overall Accuracy: {overall_accuracy:.03f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evolución del modelo\n",
    "\n",
    "En el último paso podemos ver como evoluciona el modelo a través de las épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "def draw_graph_at_epoch(epoch):\n",
    "    ax.clear()\n",
    "    plot_graph(G, graph_propagation[epoch], ax)\n",
    "    ax.set_title(f\"Epoch: {epoch+1}\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "ani = FuncAnimation(fig, draw_graph_at_epoch, frames=EPOCHS, interval=1000, repeat=True)\n",
    "plt.close()\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¡Muchas Gracias!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contacto\n",
    "\n",
    "- Twitter: [@crscardellino](https://twitter.com/crscardellino)\n",
    "- Github: [crscardellino](https://gihub.com/crscardellino)\n",
    "- Email: [ccardellino@unc.edu.ar](mailto:ccardellino@unc.edu.ar)\n",
    "- Material disponibles en [github.com/crscardellino/meetup-ds-cba-2019](https://github.com/crscardellino/meetup-ds-cba-2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Referencias\n",
    "\n",
    "- [1] Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Yu, P. S. (2019). A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596.\n",
    "- [2] Kipf, T. N., & Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907.\n",
    "- [3] Yan, S., Xiong, Y., & Lin, D. (2018, April). Spatial temporal graph convolutional networks for skeleton-based action recognition. In Thirty-Second AAAI Conference on Artificial Intelligence.\n",
    "- [4] Li, Y., Yu, R., Shahabi, C., & Liu, Y. (2017). Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926.\n",
    "- [5] Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W. L., & Leskovec, J. (2018, July). Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 974-983). ACM.\n",
    "- [6] Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., & Dahl, G. E. (2017, August). Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 1263-1272). JMLR. org.\n",
    "- [7] Jain, A., Zamir, A. R., Savarese, S., & Saxena, A. (2016). Structural-RNN: Deep learning on spatio-temporal graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 5308-5317).\n",
    "- [8] Zachary, W. W. (1977). An information flow model for conflict and fission in small groups. Journal of anthropological research, 33(4), 452-473."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
